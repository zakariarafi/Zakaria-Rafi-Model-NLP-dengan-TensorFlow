# -*- coding: utf-8 -*-
"""ZakariaRafi_Model NLP dengan TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aUKUdQHmiZXJnoYB-pcOWB2WR9m0ZWTP

Nama : Zakaria Rafi

Judul: Proyek NLP menggunakan Tensorflow
"""

import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

train_df = pd.read_csv('emotion-labels-train.csv')
val_df = pd.read_csv('emotion-labels-val.csv')
test_df = pd.read_csv('emotion-labels-test.csv')

# Menggabungkan dan mengacak data untuk memenuhi minimal 20% validation set
all_data = pd.concat([train_df, val_df, test_df]).sample(frac=1).reset_index(drop=True)

# Menghitung jumlah sampel validasi yang diperlukan untuk membuatnya 20% dari total dataset
total_samples = len(all_data)
val_samples_required = int(0.2 * total_samples)
print (f"Total sampel validasi yang dibutuhkan untuk memenuhi minimal 'Validation set sebesar 20% dari total dataset' adalah {val_samples_required}")

# Memisahkan data menjadi data latih dan data validasi
train_size = int(0.8 * len(all_data))
train_data = all_data[:train_size]
val_data = all_data[train_size:]

# Mencetak jumlah total, latih, dan validasi dari dataset
print(f"Jumlah total sampel: {total_samples}")
print(f"Jumlah sampel latihan: {len(train_data)}")
print(f"Jumlah sampel validasi: {len(val_data)}")

# Mengonfirmasi persentase set validasi
validation_percentage = (len(val_data) / total_samples) * 100
print(f"Set validasi adalah {validation_percentage:.2f}% dari total dataset.")

# Menginisialisasi tokenizer dan menggunakannya pada data latih
tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_data['text'])

# Mengubah teks menjadi urutan
train_sequences = tokenizer.texts_to_sequences(train_data['text'])
val_sequences = tokenizer.texts_to_sequences(val_data['text'])

# Padding urutan agar memiliki panjang yang sama
max_sequence_length = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in val_sequences))
train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')
val_padded = pad_sequences(val_sequences, maxlen=max_sequence_length, padding='post')

# Mengubah label menjadi integers
labels_index = {label: index for index, label in enumerate(set(train_data['label']))}
train_labels = train_data['label'].map(labels_index)
val_labels = val_data['label'].map(labels_index)

# Membangun model
model = Sequential([
    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=128, input_length=max_sequence_length),
    LSTM(128),
    Dense(len(labels_index), activation='softmax')
])

# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Mengimplementasikan callback
early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)

# Melatih model
history = model.fit(train_padded, train_labels, epochs=50,
                    validation_data=(val_padded, val_labels), callbacks=[early_stop])

# Display the validation set as 20% of the total dataset
validation_percentage = (val_samples / total_samples) * 100
print(f"Data validasi adalah {validation_percentage:.2f}% dari total dataset.")

# Evaluasi model pada data latih
train_loss, train_accuracy = model.evaluate(train_padded, train_labels, verbose=0)

# Evaluasi model pada dataset
val_loss, val_accuracy = model.evaluate(val_padded, val_labels, verbose=0)

# Print the accuracies
print(f"Training Accuracy: {train_accuracy*100:.2f}%")
print(f"Validation Accuracy: {val_accuracy*100:.2f}%")

# Check if the accuracies meet the 80% criterion
if train_accuracy >= 0.8 and val_accuracy >= 0.8:
    print("Model telah mencapai akurasi minimal 80% pada data latih dan validasi!")
else:
    print("Model belum mencapai akurasi minimal 80% pada data latih dan validasi.")

# Plot accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()